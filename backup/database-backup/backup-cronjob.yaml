# Database Backup CronJob for GetIt Bangladesh Multi-Vendor E-commerce Platform
# Amazon.com/Shopee.sg-Level Backup and Disaster Recovery

apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
  namespace: production
  labels:
    app: postgres-backup
    tier: backup
    project: getit-bangladesh
spec:
  # Run daily at 2:00 AM Bangladesh time (UTC+6 = 20:00 UTC)
  schedule: "0 20 * * *"
  timeZone: "Asia/Dhaka"
  successfulJobsHistoryLimit: 7
  failedJobsHistoryLimit: 3
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: postgres-backup
            tier: backup
            project: getit-bangladesh
        spec:
          restartPolicy: OnFailure
          serviceAccountName: backup-service-account
          securityContext:
            runAsNonRoot: true
            runAsUser: 999
            fsGroup: 999
          containers:
          - name: postgres-backup
            image: postgres:15-alpine
            imagePullPolicy: IfNotPresent
            env:
            - name: PGHOST
              value: "postgres-service"
            - name: PGPORT
              value: "5432"
            - name: PGDATABASE
              value: "getit_production"
            - name: PGUSER
              valueFrom:
                secretKeyRef:
                  name: postgres-backup-secrets
                  key: username
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-backup-secrets
                  key: password
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: backup-aws-secrets
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: backup-aws-secrets
                  key: secret-access-key
            - name: AWS_DEFAULT_REGION
              value: "ap-southeast-1"
            - name: S3_BUCKET
              value: "getit-bangladesh-backups"
            - name: BACKUP_RETENTION_DAYS
              value: "30"
            command:
            - /bin/sh
            - -c
            - |
              set -e
              
              # Install AWS CLI
              apk add --no-cache aws-cli gzip
              
              # Create backup filename with timestamp
              BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
              BACKUP_FILENAME="getit_backup_${BACKUP_DATE}.sql"
              COMPRESSED_FILENAME="${BACKUP_FILENAME}.gz"
              
              echo "Starting backup at $(date)"
              echo "Backup filename: ${BACKUP_FILENAME}"
              
              # Create database dump
              pg_dump \
                --verbose \
                --format=custom \
                --compress=9 \
                --no-owner \
                --no-privileges \
                --create \
                --clean \
                --if-exists \
                --file="/tmp/${BACKUP_FILENAME}" \
                "${PGDATABASE}"
              
              if [ $? -eq 0 ]; then
                echo "Database dump created successfully"
              else
                echo "Database dump failed"
                exit 1
              fi
              
              # Compress the backup
              gzip "/tmp/${BACKUP_FILENAME}"
              
              # Upload to S3
              echo "Uploading backup to S3..."
              aws s3 cp "/tmp/${COMPRESSED_FILENAME}" "s3://${S3_BUCKET}/postgres/daily/${COMPRESSED_FILENAME}"
              
              if [ $? -eq 0 ]; then
                echo "Backup uploaded to S3 successfully"
              else
                echo "S3 upload failed"
                exit 1
              fi
              
              # Create metadata file
              cat > "/tmp/backup_metadata_${BACKUP_DATE}.json" << EOF
              {
                "backup_date": "${BACKUP_DATE}",
                "database": "${PGDATABASE}",
                "filename": "${COMPRESSED_FILENAME}",
                "size": "$(stat -f%z /tmp/${COMPRESSED_FILENAME} 2>/dev/null || stat -c%s /tmp/${COMPRESSED_FILENAME})",
                "checksum": "$(md5sum /tmp/${COMPRESSED_FILENAME} | cut -d' ' -f1)",
                "backup_type": "full",
                "environment": "production",
                "retention_days": ${BACKUP_RETENTION_DAYS}
              }
              EOF
              
              # Upload metadata
              aws s3 cp "/tmp/backup_metadata_${BACKUP_DATE}.json" "s3://${S3_BUCKET}/postgres/metadata/backup_metadata_${BACKUP_DATE}.json"
              
              # Cleanup old backups (retain only last 30 days)
              CUTOFF_DATE=$(date -d "${BACKUP_RETENTION_DAYS} days ago" +%Y%m%d 2>/dev/null || date -v-${BACKUP_RETENTION_DAYS}d +%Y%m%d)
              echo "Cleaning up backups older than ${CUTOFF_DATE}"
              
              aws s3 ls "s3://${S3_BUCKET}/postgres/daily/" | while read -r line; do
                BACKUP_FILE=$(echo $line | awk '{print $4}')
                if [[ $BACKUP_FILE =~ getit_backup_([0-9]{8}) ]]; then
                  FILE_DATE="${BASH_REMATCH[1]}"
                  if [[ $FILE_DATE < $CUTOFF_DATE ]]; then
                    echo "Deleting old backup: $BACKUP_FILE"
                    aws s3 rm "s3://${S3_BUCKET}/postgres/daily/$BACKUP_FILE"
                    aws s3 rm "s3://${S3_BUCKET}/postgres/metadata/backup_metadata_${FILE_DATE}*.json" || true
                  fi
                fi
              done
              
              # Send notification
              echo "Backup completed successfully at $(date)"
              
              # Create backup report
              cat > "/tmp/backup_report_${BACKUP_DATE}.json" << EOF
              {
                "status": "success",
                "backup_date": "${BACKUP_DATE}",
                "database": "${PGDATABASE}",
                "filename": "${COMPRESSED_FILENAME}",
                "s3_location": "s3://${S3_BUCKET}/postgres/daily/${COMPRESSED_FILENAME}",
                "size_bytes": "$(stat -f%z /tmp/${COMPRESSED_FILENAME} 2>/dev/null || stat -c%s /tmp/${COMPRESSED_FILENAME})",
                "duration_seconds": "$(($(date +%s) - $(date -d "${BACKUP_DATE:0:8} ${BACKUP_DATE:9:2}:${BACKUP_DATE:11:2}:${BACKUP_DATE:13:2}" +%s)))",
                "environment": "production",
                "backup_type": "scheduled_daily"
              }
              EOF
              
              # Upload report
              aws s3 cp "/tmp/backup_report_${BACKUP_DATE}.json" "s3://${S3_BUCKET}/postgres/reports/backup_report_${BACKUP_DATE}.json"
              
              echo "Backup process completed successfully"
            resources:
              requests:
                memory: "512Mi"
                cpu: "200m"
              limits:
                memory: "1Gi"
                cpu: "500m"
            volumeMounts:
            - name: backup-storage
              mountPath: /tmp
          volumes:
          - name: backup-storage
            emptyDir:
              sizeLimit: 10Gi
---
# Weekly Full Backup CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-weekly-backup
  namespace: production
  labels:
    app: postgres-weekly-backup
    tier: backup
    project: getit-bangladesh
spec:
  # Run weekly on Sunday at 1:00 AM Bangladesh time
  schedule: "0 19 * * 0"
  timeZone: "Asia/Dhaka"
  successfulJobsHistoryLimit: 4
  failedJobsHistoryLimit: 2
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: postgres-weekly-backup
            tier: backup
            project: getit-bangladesh
        spec:
          restartPolicy: OnFailure
          serviceAccountName: backup-service-account
          containers:
          - name: postgres-weekly-backup
            image: postgres:15-alpine
            env:
            - name: BACKUP_TYPE
              value: "weekly"
            - name: BACKUP_RETENTION_DAYS
              value: "90"
            # ... (similar configuration as daily backup with weekly retention)
---
# Backup Verification Job
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-verification
  namespace: production
  labels:
    app: backup-verification
    tier: backup
    project: getit-bangladesh
spec:
  # Run verification daily at 4:00 AM Bangladesh time
  schedule: "0 22 * * *"
  timeZone: "Asia/Dhaka"
  successfulJobsHistoryLimit: 7
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          serviceAccountName: backup-service-account
          containers:
          - name: backup-verification
            image: postgres:15-alpine
            command:
            - /bin/sh
            - -c
            - |
              set -e
              
              # Install required tools
              apk add --no-cache aws-cli curl jq
              
              # Get latest backup
              LATEST_BACKUP=$(aws s3 ls s3://${S3_BUCKET}/postgres/daily/ | sort | tail -n 1 | awk '{print $4}')
              
              if [ -z "$LATEST_BACKUP" ]; then
                echo "No backup found for verification"
                exit 1
              fi
              
              echo "Verifying backup: $LATEST_BACKUP"
              
              # Download and verify backup integrity
              aws s3 cp "s3://${S3_BUCKET}/postgres/daily/$LATEST_BACKUP" "/tmp/$LATEST_BACKUP"
              
              # Test backup file integrity
              gunzip -t "/tmp/$LATEST_BACKUP"
              
              if [ $? -eq 0 ]; then
                echo "Backup verification successful: $LATEST_BACKUP"
              else
                echo "Backup verification failed: $LATEST_BACKUP"
                exit 1
              fi
              
              # Send verification report to monitoring
              curl -X POST http://monitoring-service:9090/api/v1/backup/verification \
                -H "Content-Type: application/json" \
                -d "{\"backup_file\":\"$LATEST_BACKUP\",\"status\":\"verified\",\"timestamp\":\"$(date -Iseconds)\"}"
            env:
            - name: S3_BUCKET
              value: "getit-bangladesh-backups"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: backup-aws-secrets
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: backup-aws-secrets
                  key: secret-access-key